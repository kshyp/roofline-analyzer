{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèîÔ∏è Roofline Analysis for Mistral Inference on T4 GPU\n",
    "\n",
    "This notebook performs roofline analysis to understand whether Mistral inference is **compute-bound** or **memory-bound** on a T4 GPU.\n",
    "\n",
    "**What you'll learn:**\n",
    "- T4 hardware limits (peak FLOPS, memory bandwidth)\n",
    "- Mistral's computational characteristics\n",
    "- Where your workload sits on the roofline\n",
    "- Optimization recommendations\n",
    "\n",
    "**Runtime:** Make sure you're using a T4 GPU: `Runtime ‚Üí Change runtime type ‚Üí T4 GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q matplotlib numpy pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "import gc\n",
    "\n",
    "# Verify GPU\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"‚ùå GPU not available! Enable it: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "print(f\"‚úÖ GPU: {gpu_name}\")\n",
    "print(f\"‚úÖ CUDA: {torch.version.cuda}\")\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "\n",
    "if \"T4\" not in gpu_name:\n",
    "    print(f\"‚ö†Ô∏è  Warning: This notebook is optimized for T4. You have: {gpu_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. T4 GPU Specifications\n",
    "\n",
    "The NVIDIA T4 is optimized for inference workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class T4Specs:\n",
    "    \"\"\"NVIDIA T4 Hardware Specifications\"\"\"\n",
    "    # Peak theoretical performance\n",
    "    peak_fp32_tflops: float = 8.1\n",
    "    peak_fp16_tflops: float = 65.0      # With Tensor Cores\n",
    "    peak_int8_tops: float = 130.0       # With Tensor Cores\n",
    "    \n",
    "    # Memory\n",
    "    memory_bandwidth_gb_s: float = 320.0\n",
    "    memory_gb: float = 16.0\n",
    "    \n",
    "    # Practical achievable performance (typically 60-80% of peak)\n",
    "    practical_efficiency: float = 0.7\n",
    "    \n",
    "    @property\n",
    "    def ridge_point_fp16(self) -> float:\n",
    "        \"\"\"FLOPs/Byte where compute meets memory bandwidth (FP16)\"\"\"\n",
    "        return (self.peak_fp16_tflops * 1e12) / (self.memory_bandwidth_gb_s * 1e9)\n",
    "    \n",
    "    @property\n",
    "    def ridge_point_fp32(self) -> float:\n",
    "        \"\"\"FLOPs/Byte where compute meets memory bandwidth (FP32)\"\"\"\n",
    "        return (self.peak_fp32_tflops * 1e12) / (self.memory_bandwidth_gb_s * 1e9)\n",
    "\n",
    "t4 = T4Specs()\n",
    "\n",
    "print(\"‚ïê\" * 50)\n",
    "print(\"NVIDIA T4 Specifications\")\n",
    "print(\"‚ïê\" * 50)\n",
    "print(f\"Peak FP32:        {t4.peak_fp32_tflops:>8.1f} TFLOPS\")\n",
    "print(f\"Peak FP16:        {t4.peak_fp16_tflops:>8.1f} TFLOPS (Tensor Cores)\")\n",
    "print(f\"Peak INT8:        {t4.peak_int8_tops:>8.1f} TOPS (Tensor Cores)\")\n",
    "print(f\"Memory BW:        {t4.memory_bandwidth_gb_s:>8.1f} GB/s\")\n",
    "print(f\"Memory:           {t4.memory_gb:>8.1f} GB\")\n",
    "print(\"‚îÄ\" * 50)\n",
    "print(f\"Ridge Point FP16: {t4.ridge_point_fp16:>8.1f} FLOPs/Byte\")\n",
    "print(f\"Ridge Point FP32: {t4.ridge_point_fp32:>8.1f} FLOPs/Byte\")\n",
    "print(\"‚ïê\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Mistral Model Configuration\n",
    "\n",
    "Define Mistral-7B architecture parameters for FLOP calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MistralConfig:\n",
    "    \"\"\"Mistral-7B Architecture Configuration\"\"\"\n",
    "    hidden_size: int = 4096\n",
    "    intermediate_size: int = 14336  # MLP intermediate\n",
    "    num_hidden_layers: int = 32\n",
    "    num_attention_heads: int = 32\n",
    "    num_key_value_heads: int = 8    # GQA (Grouped Query Attention)\n",
    "    head_dim: int = 128\n",
    "    vocab_size: int = 32000\n",
    "    max_position_embeddings: int = 32768\n",
    "    \n",
    "    # Derived\n",
    "    @property\n",
    "    def params_billions(self) -> float:\n",
    "        \"\"\"Approximate parameter count in billions\"\"\"\n",
    "        # Embedding\n",
    "        embed = self.vocab_size * self.hidden_size\n",
    "        \n",
    "        # Per layer\n",
    "        # Attention: Q, K, V projections + output projection\n",
    "        q_proj = self.hidden_size * self.hidden_size\n",
    "        k_proj = self.hidden_size * (self.num_key_value_heads * self.head_dim)\n",
    "        v_proj = self.hidden_size * (self.num_key_value_heads * self.head_dim)\n",
    "        o_proj = self.hidden_size * self.hidden_size\n",
    "        attn_per_layer = q_proj + k_proj + v_proj + o_proj\n",
    "        \n",
    "        # MLP: gate, up, down projections\n",
    "        mlp_per_layer = 3 * self.hidden_size * self.intermediate_size\n",
    "        \n",
    "        # Layer norms (small)\n",
    "        ln_per_layer = 2 * self.hidden_size\n",
    "        \n",
    "        total_per_layer = attn_per_layer + mlp_per_layer + ln_per_layer\n",
    "        \n",
    "        # LM head (often tied with embedding)\n",
    "        lm_head = self.vocab_size * self.hidden_size\n",
    "        \n",
    "        total = embed + (self.num_hidden_layers * total_per_layer) + lm_head\n",
    "        return total / 1e9\n",
    "\n",
    "mistral = MistralConfig()\n",
    "\n",
    "print(\"‚ïê\" * 50)\n",
    "print(\"Mistral-7B Architecture\")\n",
    "print(\"‚ïê\" * 50)\n",
    "print(f\"Hidden size:      {mistral.hidden_size}\")\n",
    "print(f\"Intermediate:     {mistral.intermediate_size}\")\n",
    "print(f\"Layers:           {mistral.num_hidden_layers}\")\n",
    "print(f\"Attention heads:  {mistral.num_attention_heads}\")\n",
    "print(f\"KV heads (GQA):   {mistral.num_key_value_heads}\")\n",
    "print(f\"Head dimension:   {mistral.head_dim}\")\n",
    "print(f\"Vocab size:       {mistral.vocab_size}\")\n",
    "print(\"‚îÄ\" * 50)\n",
    "print(f\"Parameters:       ~{mistral.params_billions:.2f}B\")\n",
    "print(\"‚ïê\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. FLOP Calculations for Transformer Inference\n",
    "\n",
    "Calculate FLOPs for both **prefill** (processing prompt) and **decode** (generating tokens) phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prefill_flops(config: MistralConfig, seq_len: int, batch_size: int = 1) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate FLOPs for prefill phase (processing the input prompt).\n",
    "    Uses matrix multiplications - compute bound for longer sequences.\n",
    "    \"\"\"\n",
    "    B, S, H = batch_size, seq_len, config.hidden_size\n",
    "    L = config.num_hidden_layers\n",
    "    I = config.intermediate_size\n",
    "    V = config.vocab_size\n",
    "    kv_heads = config.num_key_value_heads\n",
    "    head_dim = config.head_dim\n",
    "    \n",
    "    flops = {}\n",
    "    \n",
    "    # Per layer calculations (multiply by 2 for multiply-add)\n",
    "    # Q projection: [B, S, H] x [H, H] -> 2 * B * S * H * H\n",
    "    flops['q_proj'] = 2 * B * S * H * H * L\n",
    "    \n",
    "    # K, V projections (GQA - fewer heads)\n",
    "    kv_dim = kv_heads * head_dim\n",
    "    flops['k_proj'] = 2 * B * S * H * kv_dim * L\n",
    "    flops['v_proj'] = 2 * B * S * H * kv_dim * L\n",
    "    \n",
    "    # Attention scores: Q @ K^T -> [B, heads, S, head_dim] x [B, heads, head_dim, S]\n",
    "    flops['attn_scores'] = 2 * B * config.num_attention_heads * S * S * head_dim * L\n",
    "    \n",
    "    # Attention output: scores @ V -> [B, heads, S, S] x [B, heads, S, head_dim]\n",
    "    flops['attn_output'] = 2 * B * config.num_attention_heads * S * S * head_dim * L\n",
    "    \n",
    "    # Output projection: [B, S, H] x [H, H]\n",
    "    flops['o_proj'] = 2 * B * S * H * H * L\n",
    "    \n",
    "    # MLP: gate_proj, up_proj, down_proj (SwiGLU)\n",
    "    flops['mlp_gate'] = 2 * B * S * H * I * L\n",
    "    flops['mlp_up'] = 2 * B * S * H * I * L\n",
    "    flops['mlp_down'] = 2 * B * S * I * H * L\n",
    "    \n",
    "    # LM head (final projection to vocab)\n",
    "    flops['lm_head'] = 2 * B * S * H * V\n",
    "    \n",
    "    flops['total'] = sum(flops.values())\n",
    "    \n",
    "    return flops\n",
    "\n",
    "\n",
    "def calculate_decode_flops(config: MistralConfig, kv_cache_len: int, batch_size: int = 1) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate FLOPs for decode phase (generating one token).\n",
    "    Single token generation with KV cache - memory bound.\n",
    "    \"\"\"\n",
    "    B, H = batch_size, config.hidden_size\n",
    "    L = config.num_hidden_layers\n",
    "    I = config.intermediate_size\n",
    "    V = config.vocab_size\n",
    "    kv_heads = config.num_key_value_heads\n",
    "    head_dim = config.head_dim\n",
    "    S = kv_cache_len  # Context length for attention\n",
    "    \n",
    "    flops = {}\n",
    "    \n",
    "    # Per layer (single token, S=1 for new token)\n",
    "    # Q, K, V projections for single token\n",
    "    flops['q_proj'] = 2 * B * 1 * H * H * L\n",
    "    kv_dim = kv_heads * head_dim\n",
    "    flops['k_proj'] = 2 * B * 1 * H * kv_dim * L\n",
    "    flops['v_proj'] = 2 * B * 1 * H * kv_dim * L\n",
    "    \n",
    "    # Attention with KV cache: Q (1 token) attends to all S cached tokens\n",
    "    flops['attn_scores'] = 2 * B * config.num_attention_heads * 1 * S * head_dim * L\n",
    "    flops['attn_output'] = 2 * B * config.num_attention_heads * 1 * S * head_dim * L\n",
    "    \n",
    "    # Output projection\n",
    "    flops['o_proj'] = 2 * B * 1 * H * H * L\n",
    "    \n",
    "    # MLP\n",
    "    flops['mlp_gate'] = 2 * B * 1 * H * I * L\n",
    "    flops['mlp_up'] = 2 * B * 1 * H * I * L\n",
    "    flops['mlp_down'] = 2 * B * 1 * I * H * L\n",
    "    \n",
    "    # LM head\n",
    "    flops['lm_head'] = 2 * B * 1 * H * V\n",
    "    \n",
    "    flops['total'] = sum(flops.values())\n",
    "    \n",
    "    return flops\n",
    "\n",
    "\n",
    "# Example calculations\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"FLOP Analysis for Mistral-7B\")\n",
    "print(\"‚ïê\" * 60)\n",
    "\n",
    "for seq_len in [128, 512, 2048]:\n",
    "    prefill = calculate_prefill_flops(mistral, seq_len)\n",
    "    print(f\"\\nPrefill ({seq_len} tokens): {prefill['total']/1e12:.2f} TFLOPs\")\n",
    "\n",
    "print(\"\\n\" + \"‚îÄ\" * 60)\n",
    "\n",
    "for kv_len in [128, 512, 2048]:\n",
    "    decode = calculate_decode_flops(mistral, kv_len)\n",
    "    print(f\"Decode (1 token, KV cache={kv_len}): {decode['total']/1e9:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Memory Traffic Calculations\n",
    "\n",
    "Calculate memory bandwidth requirements for operational intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_memory_traffic(\n",
    "    config: MistralConfig,\n",
    "    seq_len: int,\n",
    "    batch_size: int = 1,\n",
    "    dtype_bytes: int = 2,  # FP16 = 2 bytes\n",
    "    phase: str = \"prefill\"\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Estimate memory traffic (bytes read + written).\n",
    "    \n",
    "    For inference, the main cost is reading model weights.\n",
    "    Each weight is read once per forward pass.\n",
    "    \"\"\"\n",
    "    B, S, H = batch_size, seq_len, config.hidden_size\n",
    "    L = config.num_hidden_layers\n",
    "    I = config.intermediate_size\n",
    "    V = config.vocab_size\n",
    "    kv_heads = config.num_key_value_heads\n",
    "    head_dim = config.head_dim\n",
    "    kv_dim = kv_heads * head_dim\n",
    "    \n",
    "    traffic = {}\n",
    "    \n",
    "    # Weight reads (read once per layer)\n",
    "    traffic['q_proj_weights'] = H * H * dtype_bytes * L\n",
    "    traffic['k_proj_weights'] = H * kv_dim * dtype_bytes * L\n",
    "    traffic['v_proj_weights'] = H * kv_dim * dtype_bytes * L\n",
    "    traffic['o_proj_weights'] = H * H * dtype_bytes * L\n",
    "    \n",
    "    traffic['mlp_gate_weights'] = H * I * dtype_bytes * L\n",
    "    traffic['mlp_up_weights'] = H * I * dtype_bytes * L\n",
    "    traffic['mlp_down_weights'] = I * H * dtype_bytes * L\n",
    "    \n",
    "    traffic['lm_head_weights'] = H * V * dtype_bytes\n",
    "    traffic['embedding_weights'] = V * H * dtype_bytes\n",
    "    \n",
    "    # Activation reads/writes (simplified - actual is complex due to recomputation)\n",
    "    if phase == \"prefill\":\n",
    "        # Activations scale with sequence length\n",
    "        traffic['activations'] = B * S * H * dtype_bytes * L * 4  # Rough estimate\n",
    "    else:  # decode\n",
    "        # KV cache read\n",
    "        kv_cache_size = 2 * B * S * kv_dim * dtype_bytes * L  # K and V\n",
    "        traffic['kv_cache'] = kv_cache_size\n",
    "        traffic['activations'] = B * 1 * H * dtype_bytes * L * 4\n",
    "    \n",
    "    traffic['total_weights'] = sum(v for k, v in traffic.items() if 'weights' in k)\n",
    "    traffic['total'] = sum(v for k, v in traffic.items() if k not in ['total_weights', 'total'])\n",
    "    \n",
    "    return traffic\n",
    "\n",
    "\n",
    "def calculate_operational_intensity(\n",
    "    config: MistralConfig,\n",
    "    seq_len: int,\n",
    "    batch_size: int = 1,\n",
    "    phase: str = \"prefill\"\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate operational intensity (FLOPs / Byte).\n",
    "    \n",
    "    Returns: (flops, bytes, operational_intensity)\n",
    "    \"\"\"\n",
    "    if phase == \"prefill\":\n",
    "        flops = calculate_prefill_flops(config, seq_len, batch_size)['total']\n",
    "    else:\n",
    "        flops = calculate_decode_flops(config, seq_len, batch_size)['total']\n",
    "    \n",
    "    memory = calculate_memory_traffic(config, seq_len, batch_size, phase=phase)['total']\n",
    "    \n",
    "    oi = flops / memory if memory > 0 else 0\n",
    "    \n",
    "    return flops, memory, oi\n",
    "\n",
    "\n",
    "# Calculate for various scenarios\n",
    "print(\"‚ïê\" * 70)\n",
    "print(\"Operational Intensity Analysis\")\n",
    "print(\"‚ïê\" * 70)\n",
    "print(f\"{'Phase':<10} {'Seq Len':<10} {'FLOPs':<15} {'Memory':<15} {'OI (F/B)':<10}\")\n",
    "print(\"‚îÄ\" * 70)\n",
    "\n",
    "scenarios = []\n",
    "\n",
    "for seq_len in [128, 256, 512, 1024, 2048]:\n",
    "    flops, mem, oi = calculate_operational_intensity(mistral, seq_len, phase=\"prefill\")\n",
    "    print(f\"{'Prefill':<10} {seq_len:<10} {flops/1e12:.2f} TFLOPs    {mem/1e9:.2f} GB       {oi:.1f}\")\n",
    "    scenarios.append(('Prefill', seq_len, flops, mem, oi))\n",
    "\n",
    "print(\"‚îÄ\" * 70)\n",
    "\n",
    "for kv_len in [128, 256, 512, 1024, 2048]:\n",
    "    flops, mem, oi = calculate_operational_intensity(mistral, kv_len, phase=\"decode\")\n",
    "    print(f\"{'Decode':<10} {kv_len:<10} {flops/1e9:.2f} GFLOPs    {mem/1e9:.2f} GB       {oi:.1f}\")\n",
    "    scenarios.append(('Decode', kv_len, flops, mem, oi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Load Model and Run Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading model in 4-bit (to fit T4 16GB)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Model loaded!\")\n",
    "print(f\"Memory used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_lengths: List[int] = [64, 128, 256, 512],\n",
    "    gen_tokens: int = 32,\n",
    "    warmup_runs: int = 2,\n",
    "    benchmark_runs: int = 5\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Benchmark prefill and decode times for various prompt lengths.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt_len in prompt_lengths:\n",
    "        print(f\"\\nBenchmarking prompt_len={prompt_len}...\")\n",
    "        \n",
    "        # Create prompt of specific length\n",
    "        base_prompt = \"Explain the following concept in detail: \"\n",
    "        padding = \"word \" * (prompt_len // 2)\n",
    "        prompt = base_prompt + padding\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=prompt_len)\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        actual_prompt_len = inputs['input_ids'].shape[1]\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(warmup_runs):\n",
    "            with torch.no_grad():\n",
    "                _ = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=gen_tokens,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.pad_token_id\n",
    "                )\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(benchmark_runs):\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=gen_tokens,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.pad_token_id\n",
    "                )\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "            times.append(end - start)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        \n",
    "        # Calculate achieved performance\n",
    "        prefill_flops = calculate_prefill_flops(mistral, actual_prompt_len)['total']\n",
    "        decode_flops = calculate_decode_flops(mistral, actual_prompt_len + gen_tokens // 2)['total'] * gen_tokens\n",
    "        total_flops = prefill_flops + decode_flops\n",
    "        \n",
    "        achieved_tflops = (total_flops / avg_time) / 1e12\n",
    "        tokens_per_sec = gen_tokens / avg_time\n",
    "        \n",
    "        result = {\n",
    "            'prompt_len': actual_prompt_len,\n",
    "            'gen_tokens': gen_tokens,\n",
    "            'avg_time': avg_time,\n",
    "            'std_time': std_time,\n",
    "            'prefill_flops': prefill_flops,\n",
    "            'decode_flops': decode_flops,\n",
    "            'total_flops': total_flops,\n",
    "            'achieved_tflops': achieved_tflops,\n",
    "            'tokens_per_sec': tokens_per_sec\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Prompt: {actual_prompt_len} tokens, Gen: {gen_tokens} tokens\")\n",
    "        print(f\"  Time: {avg_time:.3f}s ¬± {std_time:.3f}s\")\n",
    "        print(f\"  Achieved: {achieved_tflops:.2f} TFLOPS\")\n",
    "        print(f\"  Throughput: {tokens_per_sec:.1f} tokens/sec\")\n",
    "        \n",
    "        # Clear cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"Running Inference Benchmarks\")\n",
    "print(\"‚ïê\" * 60)\n",
    "\n",
    "benchmark_results = benchmark_inference(\n",
    "    model, tokenizer,\n",
    "    prompt_lengths=[64, 128, 256, 512],\n",
    "    gen_tokens=32,\n",
    "    warmup_runs=2,\n",
    "    benchmark_runs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Plot the Roofline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roofline(t4_specs: T4Specs, benchmark_results: List[Dict], mistral_config: MistralConfig):\n",
    "    \"\"\"\n",
    "    Create a roofline plot with benchmark results.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Roofline parameters\n",
    "    peak_fp16 = t4_specs.peak_fp16_tflops\n",
    "    mem_bw = t4_specs.memory_bandwidth_gb_s\n",
    "    ridge_point = t4_specs.ridge_point_fp16\n",
    "    \n",
    "    # Create roofline\n",
    "    oi_range = np.logspace(-1, 4, 500)\n",
    "    \n",
    "    # Memory-bound region: performance = bandwidth * OI\n",
    "    memory_bound = mem_bw * oi_range / 1000  # Convert to TFLOPS\n",
    "    \n",
    "    # Compute-bound region: performance = peak\n",
    "    compute_bound = np.full_like(oi_range, peak_fp16)\n",
    "    \n",
    "    # Actual roofline is minimum of both\n",
    "    roofline = np.minimum(memory_bound, compute_bound)\n",
    "    \n",
    "    # Plot roofline\n",
    "    ax.loglog(oi_range, roofline, 'b-', linewidth=3, label=f'T4 Roofline (FP16)')\n",
    "    \n",
    "    # Add practical roofline (70% efficiency)\n",
    "    practical_roofline = roofline * t4_specs.practical_efficiency\n",
    "    ax.loglog(oi_range, practical_roofline, 'b--', linewidth=2, alpha=0.5, \n",
    "              label=f'Practical (~{int(t4_specs.practical_efficiency*100)}% efficiency)')\n",
    "    \n",
    "    # Mark ridge point\n",
    "    ax.axvline(ridge_point, color='gray', linestyle=':', alpha=0.7)\n",
    "    ax.annotate(f'Ridge Point\\n({ridge_point:.0f} F/B)', \n",
    "                xy=(ridge_point, peak_fp16), \n",
    "                xytext=(ridge_point*2, peak_fp16*0.7),\n",
    "                fontsize=10, ha='left',\n",
    "                arrowprops=dict(arrowstyle='->', color='gray', alpha=0.7))\n",
    "    \n",
    "    # Plot theoretical points for prefill\n",
    "    prefill_colors = plt.cm.Greens(np.linspace(0.4, 0.9, 5))\n",
    "    for i, seq_len in enumerate([128, 256, 512, 1024, 2048]):\n",
    "        flops, mem, oi = calculate_operational_intensity(mistral_config, seq_len, phase=\"prefill\")\n",
    "        # Theoretical max performance at this OI\n",
    "        theoretical_perf = min(peak_fp16, mem_bw * oi / 1000)\n",
    "        ax.scatter([oi], [theoretical_perf * 0.5], c=[prefill_colors[i]], s=100, marker='s',\n",
    "                   edgecolors='black', linewidths=1, zorder=5)\n",
    "        ax.annotate(f'Prefill {seq_len}', xy=(oi, theoretical_perf * 0.5), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # Plot theoretical points for decode\n",
    "    decode_colors = plt.cm.Oranges(np.linspace(0.4, 0.9, 5))\n",
    "    for i, kv_len in enumerate([128, 256, 512, 1024, 2048]):\n",
    "        flops, mem, oi = calculate_operational_intensity(mistral_config, kv_len, phase=\"decode\")\n",
    "        theoretical_perf = min(peak_fp16, mem_bw * oi / 1000)\n",
    "        ax.scatter([oi], [theoretical_perf * 0.4], c=[decode_colors[i]], s=100, marker='^',\n",
    "                   edgecolors='black', linewidths=1, zorder=5)\n",
    "        ax.annotate(f'Decode {kv_len}', xy=(oi, theoretical_perf * 0.4), \n",
    "                    xytext=(5, -10), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # Plot actual benchmark results\n",
    "    if benchmark_results:\n",
    "        for result in benchmark_results:\n",
    "            # Estimate OI from benchmark\n",
    "            mem_traffic = calculate_memory_traffic(\n",
    "                mistral_config, result['prompt_len'], phase=\"prefill\"\n",
    "            )['total']\n",
    "            oi = result['total_flops'] / mem_traffic\n",
    "            \n",
    "            ax.scatter([oi], [result['achieved_tflops']], c='red', s=200, marker='*',\n",
    "                       edgecolors='darkred', linewidths=1.5, zorder=10,\n",
    "                       label=f'Measured (prompt={result[\"prompt_len\"]})')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Operational Intensity (FLOPs/Byte)', fontsize=12)\n",
    "    ax.set_ylabel('Performance (TFLOPS)', fontsize=12)\n",
    "    ax.set_title('Roofline Analysis: Mistral-7B on NVIDIA T4', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(0.1, 10000)\n",
    "    ax.set_ylim(0.1, 100)\n",
    "    \n",
    "    ax.grid(True, which='both', alpha=0.3)\n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    \n",
    "    # Add annotations for regions\n",
    "    ax.text(0.5, 0.3, 'Memory\\nBound', fontsize=14, alpha=0.5, ha='center', transform=ax.transAxes)\n",
    "    ax.text(0.85, 0.7, 'Compute\\nBound', fontsize=14, alpha=0.5, ha='center', transform=ax.transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Create the roofline plot\n",
    "fig = plot_roofline(t4, benchmark_results, mistral)\n",
    "plt.savefig('roofline_mistral_t4.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Plot saved as 'roofline_mistral_t4.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Detailed Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_breakdown(benchmark_results: List[Dict]):\n",
    "    \"\"\"\n",
    "    Visualize performance metrics from benchmarks.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    prompt_lens = [r['prompt_len'] for r in benchmark_results]\n",
    "    \n",
    "    # 1. Throughput vs Prompt Length\n",
    "    ax1 = axes[0, 0]\n",
    "    throughputs = [r['tokens_per_sec'] for r in benchmark_results]\n",
    "    ax1.bar(range(len(prompt_lens)), throughputs, color='steelblue', edgecolor='black')\n",
    "    ax1.set_xticks(range(len(prompt_lens)))\n",
    "    ax1.set_xticklabels(prompt_lens)\n",
    "    ax1.set_xlabel('Prompt Length (tokens)')\n",
    "    ax1.set_ylabel('Throughput (tokens/sec)')\n",
    "    ax1.set_title('Generation Throughput vs Prompt Length')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Achieved TFLOPS\n",
    "    ax2 = axes[0, 1]\n",
    "    achieved = [r['achieved_tflops'] for r in benchmark_results]\n",
    "    ax2.bar(range(len(prompt_lens)), achieved, color='coral', edgecolor='black')\n",
    "    ax2.axhline(t4.peak_fp16_tflops, color='red', linestyle='--', label=f'T4 Peak FP16 ({t4.peak_fp16_tflops} TFLOPS)')\n",
    "    ax2.axhline(t4.peak_fp16_tflops * 0.7, color='orange', linestyle=':', label='70% efficiency')\n",
    "    ax2.set_xticks(range(len(prompt_lens)))\n",
    "    ax2.set_xticklabels(prompt_lens)\n",
    "    ax2.set_xlabel('Prompt Length (tokens)')\n",
    "    ax2.set_ylabel('Achieved Performance (TFLOPS)')\n",
    "    ax2.set_title('Achieved vs Peak Performance')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. Time Breakdown\n",
    "    ax3 = axes[1, 0]\n",
    "    times = [r['avg_time'] for r in benchmark_results]\n",
    "    stds = [r['std_time'] for r in benchmark_results]\n",
    "    ax3.bar(range(len(prompt_lens)), times, yerr=stds, color='mediumseagreen', \n",
    "            edgecolor='black', capsize=5)\n",
    "    ax3.set_xticks(range(len(prompt_lens)))\n",
    "    ax3.set_xticklabels(prompt_lens)\n",
    "    ax3.set_xlabel('Prompt Length (tokens)')\n",
    "    ax3.set_ylabel('Inference Time (seconds)')\n",
    "    ax3.set_title('Total Inference Time')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. FLOP Breakdown\n",
    "    ax4 = axes[1, 1]\n",
    "    prefill_flops = [r['prefill_flops']/1e12 for r in benchmark_results]\n",
    "    decode_flops = [r['decode_flops']/1e12 for r in benchmark_results]\n",
    "    \n",
    "    x = np.arange(len(prompt_lens))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax4.bar(x - width/2, prefill_flops, width, label='Prefill', color='royalblue', edgecolor='black')\n",
    "    ax4.bar(x + width/2, decode_flops, width, label='Decode', color='darkorange', edgecolor='black')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(prompt_lens)\n",
    "    ax4.set_xlabel('Prompt Length (tokens)')\n",
    "    ax4.set_ylabel('FLOPs (TFLOPS)')\n",
    "    ax4.set_title('FLOP Distribution: Prefill vs Decode')\n",
    "    ax4.legend()\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "if benchmark_results:\n",
    "    fig = plot_performance_breakdown(benchmark_results)\n",
    "    plt.savefig('performance_breakdown.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Analysis Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis_report(t4_specs: T4Specs, benchmark_results: List[Dict], mistral_config: MistralConfig):\n",
    "    \"\"\"\n",
    "    Generate a summary report with optimization recommendations.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"‚ïê\" * 70)\n",
    "    print(\"ROOFLINE ANALYSIS REPORT: Mistral-7B on NVIDIA T4\")\n",
    "    print(\"‚ïê\" * 70)\n",
    "    \n",
    "    # Key findings\n",
    "    print(\"\\nüìä KEY FINDINGS\")\n",
    "    print(\"‚îÄ\" * 70)\n",
    "    \n",
    "    # Calculate average efficiency\n",
    "    if benchmark_results:\n",
    "        avg_achieved = np.mean([r['achieved_tflops'] for r in benchmark_results])\n",
    "        efficiency = avg_achieved / t4_specs.peak_fp16_tflops * 100\n",
    "        avg_throughput = np.mean([r['tokens_per_sec'] for r in benchmark_results])\n",
    "        \n",
    "        print(f\"  Average achieved performance: {avg_achieved:.2f} TFLOPS\")\n",
    "        print(f\"  Hardware utilization: {efficiency:.1f}% of peak FP16\")\n",
    "        print(f\"  Average throughput: {avg_throughput:.1f} tokens/sec\")\n",
    "    \n",
    "    # Bottleneck analysis\n",
    "    print(\"\\nüîç BOTTLENECK ANALYSIS\")\n",
    "    print(\"‚îÄ\" * 70)\n",
    "    \n",
    "    # Check decode OI\n",
    "    _, _, decode_oi = calculate_operational_intensity(mistral_config, 512, phase=\"decode\")\n",
    "    _, _, prefill_oi = calculate_operational_intensity(mistral_config, 512, phase=\"prefill\")\n",
    "    \n",
    "    print(f\"  Prefill OI (512 tokens): {prefill_oi:.1f} FLOPs/Byte\")\n",
    "    print(f\"  Decode OI (512 KV cache): {decode_oi:.1f} FLOPs/Byte\")\n",
    "    print(f\"  T4 Ridge Point (FP16): {t4_specs.ridge_point_fp16:.1f} FLOPs/Byte\")\n",
    "    \n",
    "    if decode_oi < t4_specs.ridge_point_fp16:\n",
    "        print(\"\\n  ‚ö†Ô∏è  DECODE PHASE IS MEMORY-BOUND\")\n",
    "        print(\"      The decode phase (token generation) is limited by memory bandwidth.\")\n",
    "        print(\"      This is typical for autoregressive LLM inference.\")\n",
    "    \n",
    "    if prefill_oi > t4_specs.ridge_point_fp16:\n",
    "        print(\"\\n  ‚úÖ PREFILL PHASE IS COMPUTE-BOUND (for longer prompts)\")\n",
    "        print(\"      The prefill phase can better utilize Tensor Cores.\")\n",
    "    \n",
    "    # Optimization recommendations\n",
    "    print(\"\\nüí° OPTIMIZATION RECOMMENDATIONS\")\n",
    "    print(\"‚îÄ\" * 70)\n",
    "    \n",
    "    recommendations = [\n",
    "        (\"Increase Batch Size\", \n",
    "         \"Batching multiple requests improves arithmetic intensity and GPU utilization.\",\n",
    "         \"Memory-bound decode becomes more efficient with larger batches.\"),\n",
    "        \n",
    "        (\"Use Quantization\",\n",
    "         \"INT8/INT4 quantization reduces memory traffic and increases effective bandwidth.\",\n",
    "         \"T4 has 130 INT8 TOPS vs 65 FP16 TFLOPS - 2x theoretical improvement.\"),\n",
    "        \n",
    "        (\"Enable Flash Attention\",\n",
    "         \"Reduces memory traffic for attention computation.\",\n",
    "         \"Fuses operations to minimize HBM access.\"),\n",
    "        \n",
    "        (\"Use Continuous Batching\",\n",
    "         \"Dynamically batch requests to maximize GPU utilization.\",\n",
    "         \"Frameworks: vLLM, TensorRT-LLM, Text Generation Inference.\"),\n",
    "        \n",
    "        (\"KV Cache Optimization\",\n",
    "         \"PagedAttention reduces memory fragmentation.\",\n",
    "         \"Allows serving more concurrent requests.\"),\n",
    "        \n",
    "        (\"Speculative Decoding\",\n",
    "         \"Use a smaller draft model to predict multiple tokens.\",\n",
    "         \"Can improve throughput by 2-3x for memory-bound scenarios.\")\n",
    "    ]\n",
    "    \n",
    "    for i, (title, desc, detail) in enumerate(recommendations, 1):\n",
    "        print(f\"\\n  {i}. {title}\")\n",
    "        print(f\"     {desc}\")\n",
    "        print(f\"     ‚Üí {detail}\")\n",
    "    \n",
    "    # Hardware comparison\n",
    "    print(\"\\nüìà HARDWARE SCALING NOTES\")\n",
    "    print(\"‚îÄ\" * 70)\n",
    "    print(\"  For memory-bound workloads like LLM decode:\")\n",
    "    print(\"  ‚Ä¢ A100 (1.5 TB/s BW) would be ~4.7x faster than T4 (320 GB/s)\")\n",
    "    print(\"  ‚Ä¢ H100 (3.35 TB/s BW) would be ~10x faster than T4\")\n",
    "    print(\"  ‚Ä¢ For compute-bound prefill, gains scale with TFLOPS instead\")\n",
    "    \n",
    "    print(\"\\n\" + \"‚ïê\" * 70)\n",
    "    print(\"END OF REPORT\")\n",
    "    print(\"‚ïê\" * 70)\n",
    "\n",
    "generate_analysis_report(t4, benchmark_results, mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Interactive Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive: Test different batch sizes (if memory allows)\n",
    "\n",
    "def explore_batch_scaling(model, tokenizer, batch_sizes=[1, 2, 4], prompt_len=128, gen_tokens=16):\n",
    "    \"\"\"\n",
    "    Explore how batch size affects throughput.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"‚ïê\" * 60)\n",
    "    print(\"BATCH SIZE SCALING ANALYSIS\")\n",
    "    print(\"‚ïê\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        try:\n",
    "            # Create batched input\n",
    "            prompt = \"Explain machine learning in simple terms. \" * (prompt_len // 10)\n",
    "            inputs = tokenizer([prompt] * batch_size, return_tensors=\"pt\", \n",
    "                              padding=True, truncation=True, max_length=prompt_len)\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            \n",
    "            # Warmup\n",
    "            with torch.no_grad():\n",
    "                _ = model.generate(**inputs, max_new_tokens=gen_tokens, do_sample=False,\n",
    "                                   pad_token_id=tokenizer.pad_token_id)\n",
    "            \n",
    "            # Benchmark\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _ = model.generate(**inputs, max_new_tokens=gen_tokens, do_sample=False,\n",
    "                                   pad_token_id=tokenizer.pad_token_id)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = time.perf_counter() - start\n",
    "            \n",
    "            total_tokens = batch_size * gen_tokens\n",
    "            throughput = total_tokens / elapsed\n",
    "            \n",
    "            results.append({\n",
    "                'batch_size': batch_size,\n",
    "                'time': elapsed,\n",
    "                'throughput': throughput\n",
    "            })\n",
    "            \n",
    "            print(f\"  Batch {batch_size}: {throughput:.1f} tokens/sec ({elapsed:.3f}s for {total_tokens} tokens)\")\n",
    "            \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(f\"  Batch {batch_size}: ‚ùå OOM - not enough GPU memory\")\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    if len(results) > 1:\n",
    "        scaling = results[-1]['throughput'] / results[0]['throughput']\n",
    "        print(f\"\\n  Throughput scaling (batch {results[-1]['batch_size']} vs {results[0]['batch_size']}): {scaling:.2f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to run batch scaling analysis\n",
    "# batch_results = explore_batch_scaling(model, tokenizer, batch_sizes=[1, 2], prompt_len=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **T4 Hardware Limits**: 65 TFLOPS (FP16), 320 GB/s memory bandwidth\n",
    "2. **Roofline Analysis**: Visualized compute vs memory boundedness\n",
    "3. **Key Insight**: LLM decode is **memory-bound** on T4 (and most GPUs)\n",
    "4. **Optimizations**: Batching, quantization, and specialized inference engines help\n",
    "\n",
    "For production deployment, consider:\n",
    "- **vLLM** or **TensorRT-LLM** for optimized inference\n",
    "- **Quantization** (AWQ, GPTQ) for better memory efficiency\n",
    "- **Larger GPUs** (A100, H100) if memory bandwidth is the bottleneck"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
