{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X39uNvwmX1vM"
      },
      "source": [
        "# üèîÔ∏è Roofline Analysis for Mistral Inference on T4 GPU\n",
        "\n",
        "This notebook performs roofline analysis to understand whether Mistral inference is **compute-bound** or **memory-bound** on a T4 GPU.\n",
        "\n",
        "**What you'll learn:**\n",
        "- T4 hardware limits (peak FLOPS, memory bandwidth)\n",
        "- Mistral's computational characteristics\n",
        "- Where your workload sits on the roofline\n",
        "- Optimization recommendations\n",
        "\n",
        "**Runtime:** Make sure you're using a T4 GPU: `Runtime ‚Üí Change runtime type ‚Üí T4 GPU`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo0t9pujX1vN"
      },
      "source": [
        "---\n",
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKOI9DOmX1vO",
        "outputId": "4ea8a9cf-0a9d-4fac-ff68-6428b6dacf42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q matplotlib numpy pandas seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_6l_wy9X1vO",
        "outputId": "d0585039-4d95-4554-f786-ed90f1291916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU: Tesla T4\n",
            "‚úÖ CUDA: 12.8\n",
            "‚úÖ PyTorch: 2.10.0+cu128\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "import gc\n",
        "\n",
        "# Verify GPU\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"‚ùå GPU not available! Enable it: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "print(f\"‚úÖ GPU: {gpu_name}\")\n",
        "print(f\"‚úÖ CUDA: {torch.version.cuda}\")\n",
        "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
        "\n",
        "if \"T4\" not in gpu_name:\n",
        "    print(f\"‚ö†Ô∏è  Warning: This notebook is optimized for T4. You have: {gpu_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM8M5KrRX1vP"
      },
      "source": [
        "---\n",
        "## 2. T4 GPU Specifications\n",
        "\n",
        "The NVIDIA T4 is optimized for inference workloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejXmOARaX1vP",
        "outputId": "1a866833-15ba-4cdf-fde7-ba5f9308aa3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "NVIDIA T4 Specifications\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "Peak FP32:             8.1 TFLOPS\n",
            "Peak FP16:            65.0 TFLOPS (Tensor Cores)\n",
            "Peak INT8:           130.0 TOPS (Tensor Cores)\n",
            "Memory BW:           320.0 GB/s\n",
            "Memory:               16.0 GB\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Ridge Point FP16:    203.1 FLOPs/Byte\n",
            "Ridge Point FP32:     25.3 FLOPs/Byte\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class T4Specs:\n",
        "    \"\"\"NVIDIA T4 Hardware Specifications\"\"\"\n",
        "    # Peak theoretical performance\n",
        "    peak_fp32_tflops: float = 8.1\n",
        "    peak_fp16_tflops: float = 65.0      # With Tensor Cores\n",
        "    peak_int8_tops: float = 130.0       # With Tensor Cores\n",
        "\n",
        "    # Memory\n",
        "    memory_bandwidth_gb_s: float = 320.0\n",
        "    memory_gb: float = 16.0\n",
        "\n",
        "    # Practical achievable performance (typically 60-80% of peak)\n",
        "    practical_efficiency: float = 0.7\n",
        "\n",
        "    @property\n",
        "    def ridge_point_fp16(self) -> float:\n",
        "        \"\"\"FLOPs/Byte where compute meets memory bandwidth (FP16)\"\"\"\n",
        "        return (self.peak_fp16_tflops * 1e12) / (self.memory_bandwidth_gb_s * 1e9)\n",
        "\n",
        "    @property\n",
        "    def ridge_point_fp32(self) -> float:\n",
        "        \"\"\"FLOPs/Byte where compute meets memory bandwidth (FP32)\"\"\"\n",
        "        return (self.peak_fp32_tflops * 1e12) / (self.memory_bandwidth_gb_s * 1e9)\n",
        "\n",
        "t4 = T4Specs()\n",
        "\n",
        "print(\"‚ïê\" * 50)\n",
        "print(\"NVIDIA T4 Specifications\")\n",
        "print(\"‚ïê\" * 50)\n",
        "print(f\"Peak FP32:        {t4.peak_fp32_tflops:>8.1f} TFLOPS\")\n",
        "print(f\"Peak FP16:        {t4.peak_fp16_tflops:>8.1f} TFLOPS (Tensor Cores)\")\n",
        "print(f\"Peak INT8:        {t4.peak_int8_tops:>8.1f} TOPS (Tensor Cores)\")\n",
        "print(f\"Memory BW:        {t4.memory_bandwidth_gb_s:>8.1f} GB/s\")\n",
        "print(f\"Memory:           {t4.memory_gb:>8.1f} GB\")\n",
        "print(\"‚îÄ\" * 50)\n",
        "print(f\"Ridge Point FP16: {t4.ridge_point_fp16:>8.1f} FLOPs/Byte\")\n",
        "print(f\"Ridge Point FP32: {t4.ridge_point_fp32:>8.1f} FLOPs/Byte\")\n",
        "print(\"‚ïê\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91zuKsn2X1vP"
      },
      "source": [
        "---\n",
        "## 3. Mistral Model Configuration\n",
        "\n",
        "Define Mistral-7B architecture parameters for FLOP calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjLY7Wy3X1vP",
        "outputId": "32f26094-8b65-453f-d736-fdd50148f8f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "Mistral-7B Architecture\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "Hidden size:      4096\n",
            "Intermediate:     14336\n",
            "Layers:           32\n",
            "Attention heads:  32\n",
            "KV heads (GQA):   8\n",
            "Head dimension:   128\n",
            "Vocab size:       32000\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Parameters:       ~7.24B\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class MistralConfig:\n",
        "    \"\"\"Mistral-7B Architecture Configuration\"\"\"\n",
        "    hidden_size: int = 4096\n",
        "    intermediate_size: int = 14336  # MLP intermediate\n",
        "    num_hidden_layers: int = 32\n",
        "    num_attention_heads: int = 32\n",
        "    num_key_value_heads: int = 8    # GQA (Grouped Query Attention)\n",
        "    head_dim: int = 128\n",
        "    vocab_size: int = 32000\n",
        "    max_position_embeddings: int = 32768\n",
        "\n",
        "    # Derived\n",
        "    @property\n",
        "    def params_billions(self) -> float:\n",
        "        \"\"\"Approximate parameter count in billions\"\"\"\n",
        "        # Embedding\n",
        "        embed = self.vocab_size * self.hidden_size\n",
        "\n",
        "        # Per layer\n",
        "        # Attention: Q, K, V projections + output projection\n",
        "        q_proj = self.hidden_size * self.hidden_size\n",
        "        k_proj = self.hidden_size * (self.num_key_value_heads * self.head_dim)\n",
        "        v_proj = self.hidden_size * (self.num_key_value_heads * self.head_dim)\n",
        "        o_proj = self.hidden_size * self.hidden_size\n",
        "        attn_per_layer = q_proj + k_proj + v_proj + o_proj\n",
        "\n",
        "        # MLP: gate, up, down projections\n",
        "        mlp_per_layer = 3 * self.hidden_size * self.intermediate_size\n",
        "\n",
        "        # Layer norms (small)\n",
        "        ln_per_layer = 2 * self.hidden_size\n",
        "\n",
        "        total_per_layer = attn_per_layer + mlp_per_layer + ln_per_layer\n",
        "\n",
        "        # LM head (often tied with embedding)\n",
        "        lm_head = self.vocab_size * self.hidden_size\n",
        "\n",
        "        total = embed + (self.num_hidden_layers * total_per_layer) + lm_head\n",
        "        return total / 1e9\n",
        "\n",
        "mistral = MistralConfig()\n",
        "\n",
        "print(\"‚ïê\" * 50)\n",
        "print(\"Mistral-7B Architecture\")\n",
        "print(\"‚ïê\" * 50)\n",
        "print(f\"Hidden size:      {mistral.hidden_size}\")\n",
        "print(f\"Intermediate:     {mistral.intermediate_size}\")\n",
        "print(f\"Layers:           {mistral.num_hidden_layers}\")\n",
        "print(f\"Attention heads:  {mistral.num_attention_heads}\")\n",
        "print(f\"KV heads (GQA):   {mistral.num_key_value_heads}\")\n",
        "print(f\"Head dimension:   {mistral.head_dim}\")\n",
        "print(f\"Vocab size:       {mistral.vocab_size}\")\n",
        "print(\"‚îÄ\" * 50)\n",
        "print(f\"Parameters:       ~{mistral.params_billions:.2f}B\")\n",
        "print(\"‚ïê\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lp1CeWWX1vQ"
      },
      "source": [
        "---\n",
        "## 4. FLOP Calculations for Transformer Inference\n",
        "\n",
        "Calculate FLOPs for both **prefill** (processing prompt) and **decode** (generating tokens) phases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBPc2GsqX1vQ",
        "outputId": "97c07503-1379-498e-d389-e7cebcf471c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "FLOP Analysis for Mistral-7B\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "\n",
            "Prefill (128 tokens): 1.83 TFLOPs\n",
            "\n",
            "Prefill (512 tokens): 7.42 TFLOPs\n",
            "\n",
            "Prefill (2048 tokens): 31.32 TFLOPs\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Decode (1 token, KV cache=128): 14.29 GFLOPs\n",
            "Decode (1 token, KV cache=512): 14.49 GFLOPs\n",
            "Decode (1 token, KV cache=2048): 15.29 GFLOPs\n"
          ]
        }
      ],
      "source": [
        "def calculate_prefill_flops(config: MistralConfig, seq_len: int, batch_size: int = 1) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate FLOPs for prefill phase (processing the input prompt).\n",
        "    Uses matrix multiplications - compute bound for longer sequences.\n",
        "    \"\"\"\n",
        "    B, S, H = batch_size, seq_len, config.hidden_size\n",
        "    L = config.num_hidden_layers\n",
        "    I = config.intermediate_size\n",
        "    V = config.vocab_size\n",
        "    kv_heads = config.num_key_value_heads\n",
        "    head_dim = config.head_dim\n",
        "\n",
        "    flops = {}\n",
        "\n",
        "    # Per layer calculations (multiply by 2 for multiply-add)\n",
        "    # Q projection: [B, S, H] x [H, H] -> 2 * B * S * H * H\n",
        "    flops['q_proj'] = 2 * B * S * H * H * L\n",
        "\n",
        "    # K, V projections (GQA - fewer heads)\n",
        "    kv_dim = kv_heads * head_dim\n",
        "    flops['k_proj'] = 2 * B * S * H * kv_dim * L\n",
        "    flops['v_proj'] = 2 * B * S * H * kv_dim * L\n",
        "\n",
        "    # Attention scores: Q @ K^T -> [B, heads, S, head_dim] x [B, heads, head_dim, S]\n",
        "    flops['attn_scores'] = 2 * B * config.num_attention_heads * S * S * head_dim * L\n",
        "\n",
        "    # Attention output: scores @ V -> [B, heads, S, S] x [B, heads, S, head_dim]\n",
        "    flops['attn_output'] = 2 * B * config.num_attention_heads * S * S * head_dim * L\n",
        "\n",
        "    # Output projection: [B, S, H] x [H, H]\n",
        "    flops['o_proj'] = 2 * B * S * H * H * L\n",
        "\n",
        "    # MLP: gate_proj, up_proj, down_proj (SwiGLU)\n",
        "    flops['mlp_gate'] = 2 * B * S * H * I * L\n",
        "    flops['mlp_up'] = 2 * B * S * H * I * L\n",
        "    flops['mlp_down'] = 2 * B * S * I * H * L\n",
        "\n",
        "    # LM head (final projection to vocab)\n",
        "    flops['lm_head'] = 2 * B * S * H * V\n",
        "\n",
        "    flops['total'] = sum(flops.values())\n",
        "\n",
        "    return flops\n",
        "\n",
        "\n",
        "def calculate_decode_flops(config: MistralConfig, kv_cache_len: int, batch_size: int = 1) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate FLOPs for decode phase (generating one token).\n",
        "    Single token generation with KV cache - memory bound.\n",
        "    \"\"\"\n",
        "    B, H = batch_size, config.hidden_size\n",
        "    L = config.num_hidden_layers\n",
        "    I = config.intermediate_size\n",
        "    V = config.vocab_size\n",
        "    kv_heads = config.num_key_value_heads\n",
        "    head_dim = config.head_dim\n",
        "    S = kv_cache_len  # Context length for attention\n",
        "\n",
        "    flops = {}\n",
        "\n",
        "    # Per layer (single token, S=1 for new token)\n",
        "    # Q, K, V projections for single token\n",
        "    flops['q_proj'] = 2 * B * 1 * H * H * L\n",
        "    kv_dim = kv_heads * head_dim\n",
        "    flops['k_proj'] = 2 * B * 1 * H * kv_dim * L\n",
        "    flops['v_proj'] = 2 * B * 1 * H * kv_dim * L\n",
        "\n",
        "    # Attention with KV cache: Q (1 token) attends to all S cached tokens\n",
        "    flops['attn_scores'] = 2 * B * config.num_attention_heads * 1 * S * head_dim * L\n",
        "    flops['attn_output'] = 2 * B * config.num_attention_heads * 1 * S * head_dim * L\n",
        "\n",
        "    # Output projection\n",
        "    flops['o_proj'] = 2 * B * 1 * H * H * L\n",
        "\n",
        "    # MLP\n",
        "    flops['mlp_gate'] = 2 * B * 1 * H * I * L\n",
        "    flops['mlp_up'] = 2 * B * 1 * H * I * L\n",
        "    flops['mlp_down'] = 2 * B * 1 * I * H * L\n",
        "\n",
        "    # LM head\n",
        "    flops['lm_head'] = 2 * B * 1 * H * V\n",
        "\n",
        "    flops['total'] = sum(flops.values())\n",
        "\n",
        "    return flops\n",
        "\n",
        "\n",
        "# Example calculations\n",
        "print(\"‚ïê\" * 60)\n",
        "print(\"FLOP Analysis for Mistral-7B\")\n",
        "print(\"‚ïê\" * 60)\n",
        "\n",
        "for seq_len in [128, 512, 2048]:\n",
        "    prefill = calculate_prefill_flops(mistral, seq_len)\n",
        "    print(f\"\\nPrefill ({seq_len} tokens): {prefill['total']/1e12:.2f} TFLOPs\")\n",
        "\n",
        "print(\"\\n\" + \"‚îÄ\" * 60)\n",
        "\n",
        "for kv_len in [128, 512, 2048]:\n",
        "    decode = calculate_decode_flops(mistral, kv_len)\n",
        "    print(f\"Decode (1 token, KV cache={kv_len}): {decode['total']/1e9:.2f} GFLOPs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jykQUkzLX1vR"
      },
      "source": [
        "---\n",
        "## 5. Memory Traffic Calculations\n",
        "\n",
        "Calculate memory bandwidth requirements for operational intensity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_tkQ4z3X1vR",
        "outputId": "dafb04cf-dbc9-4fe5-bfed-4ac886117903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "Operational Intensity Analysis\n",
            "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
            "Phase      Seq Len    FLOPs           Memory          OI (F/B)  \n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Prefill    128        1.83 TFLOPs    14.62 GB       125.1\n",
            "Prefill    256        3.67 TFLOPs    14.75 GB       249.1\n",
            "Prefill    512        7.42 TFLOPs    15.02 GB       493.9\n",
            "Prefill    1024       15.11 TFLOPs    15.56 GB       971.4\n",
            "Prefill    2048       31.32 TFLOPs    16.63 GB       1883.5\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Decode     128        14.29 GFLOPs    14.50 GB       1.0\n",
            "Decode     256        14.36 GFLOPs    14.52 GB       1.0\n",
            "Decode     512        14.49 GFLOPs    14.55 GB       1.0\n",
            "Decode     1024       14.76 GFLOPs    14.62 GB       1.0\n",
            "Decode     2048       15.29 GFLOPs    14.75 GB       1.0\n"
          ]
        }
      ],
      "source": [
        "def calculate_memory_traffic(\n",
        "    config: MistralConfig,\n",
        "    seq_len: int,\n",
        "    batch_size: int = 1,\n",
        "    dtype_bytes: int = 2,  # FP16 = 2 bytes\n",
        "    phase: str = \"prefill\"\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Estimate memory traffic (bytes read + written).\n",
        "\n",
        "    For inference, the main cost is reading model weights.\n",
        "    Each weight is read once per forward pass.\n",
        "    \"\"\"\n",
        "    B, S, H = batch_size, seq_len, config.hidden_size\n",
        "    L = config.num_hidden_layers\n",
        "    I = config.intermediate_size\n",
        "    V = config.vocab_size\n",
        "    kv_heads = config.num_key_value_heads\n",
        "    head_dim = config.head_dim\n",
        "    kv_dim = kv_heads * head_dim\n",
        "\n",
        "    traffic = {}\n",
        "\n",
        "    # Weight reads (read once per layer)\n",
        "    traffic['q_proj_weights'] = H * H * dtype_bytes * L\n",
        "    traffic['k_proj_weights'] = H * kv_dim * dtype_bytes * L\n",
        "    traffic['v_proj_weights'] = H * kv_dim * dtype_bytes * L\n",
        "    traffic['o_proj_weights'] = H * H * dtype_bytes * L\n",
        "\n",
        "    traffic['mlp_gate_weights'] = H * I * dtype_bytes * L\n",
        "    traffic['mlp_up_weights'] = H * I * dtype_bytes * L\n",
        "    traffic['mlp_down_weights'] = I * H * dtype_bytes * L\n",
        "\n",
        "    traffic['lm_head_weights'] = H * V * dtype_bytes\n",
        "    traffic['embedding_weights'] = V * H * dtype_bytes\n",
        "\n",
        "    # Activation reads/writes (simplified - actual is complex due to recomputation)\n",
        "    if phase == \"prefill\":\n",
        "        # Activations scale with sequence length\n",
        "        traffic['activations'] = B * S * H * dtype_bytes * L * 4  # Rough estimate\n",
        "    else:  # decode\n",
        "        # KV cache read\n",
        "        kv_cache_size = 2 * B * S * kv_dim * dtype_bytes * L  # K and V\n",
        "        traffic['kv_cache'] = kv_cache_size\n",
        "        traffic['activations'] = B * 1 * H * dtype_bytes * L * 4\n",
        "\n",
        "    traffic['total_weights'] = sum(v for k, v in traffic.items() if 'weights' in k)\n",
        "    traffic['total'] = sum(v for k, v in traffic.items() if k not in ['total_weights', 'total'])\n",
        "\n",
        "    return traffic\n",
        "\n",
        "\n",
        "def calculate_operational_intensity(\n",
        "    config: MistralConfig,\n",
        "    seq_len: int,\n",
        "    batch_size: int = 1,\n",
        "    phase: str = \"prefill\"\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Calculate operational intensity (FLOPs / Byte).\n",
        "\n",
        "    Returns: (flops, bytes, operational_intensity)\n",
        "    \"\"\"\n",
        "    if phase == \"prefill\":\n",
        "        flops = calculate_prefill_flops(config, seq_len, batch_size)['total']\n",
        "    else:\n",
        "        flops = calculate_decode_flops(config, seq_len, batch_size)['total']\n",
        "\n",
        "    memory = calculate_memory_traffic(config, seq_len, batch_size, phase=phase)['total']\n",
        "\n",
        "    oi = flops / memory if memory > 0 else 0\n",
        "\n",
        "    return flops, memory, oi\n",
        "\n",
        "\n",
        "# Calculate for various scenarios\n",
        "print(\"‚ïê\" * 70)\n",
        "print(\"Operational Intensity Analysis\")\n",
        "print(\"‚ïê\" * 70)\n",
        "print(f\"{'Phase':<10} {'Seq Len':<10} {'FLOPs':<15} {'Memory':<15} {'OI (F/B)':<10}\")\n",
        "print(\"‚îÄ\" * 70)\n",
        "\n",
        "scenarios = []\n",
        "\n",
        "for seq_len in [128, 256, 512, 1024, 2048]:\n",
        "    flops, mem, oi = calculate_operational_intensity(mistral, seq_len, phase=\"prefill\")\n",
        "    print(f\"{'Prefill':<10} {seq_len:<10} {flops/1e12:.2f} TFLOPs    {mem/1e9:.2f} GB       {oi:.1f}\")\n",
        "    scenarios.append(('Prefill', seq_len, flops, mem, oi))\n",
        "\n",
        "print(\"‚îÄ\" * 70)\n",
        "\n",
        "for kv_len in [128, 256, 512, 1024, 2048]:\n",
        "    flops, mem, oi = calculate_operational_intensity(mistral, kv_len, phase=\"decode\")\n",
        "    print(f\"{'Decode':<10} {kv_len:<10} {flops/1e9:.2f} GFLOPs    {mem/1e9:.2f} GB       {oi:.1f}\")\n",
        "    scenarios.append(('Decode', kv_len, flops, mem, oi))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1__LSITsX1vR"
      },
      "source": [
        "---\n",
        "## 6. Load Model and Run Benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "54a0434cba2648babf5eaf171eb10a74",
            "4357ca5b0211472c8946df0b950710ef",
            "3d148d4880ac4578a9ab6b3f9a2eda8e",
            "97873d730ca84a9c9fe1e92579c18d96",
            "de2190c7d2504b5d84ef276aca25eed7",
            "c8f377a4f091472398f9cea69a05ebda",
            "ce47db5bf14d42de8b6f806ee1071710",
            "78e2c9be96ac42799e4a2ca3f8576b51",
            "44008ada972d4e5a89e81110bc27fa19",
            "6c683b57f6664ba4842aa26c8f08a485",
            "7193531b1ef34df8afd64d27b67e0dd4"
          ]
        },
        "id": "NS8n1yQwX1vR",
        "outputId": "061a5ccf-9102-4707-d73d-b683069363c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer...\n",
            "Loading model in 4-bit (to fit T4 16GB)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54a0434cba2648babf5eaf171eb10a74"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loading model in 4-bit (to fit T4 16GB)...\")\n",
        "\n",
        "# Define the quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16, # This sets the dtype for non-quantized parts\n",
        "    quantization_config=bnb_config, # Pass the bnb_config here\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(f\"‚úÖ Model loaded!\")\n",
        "print(f\"Memory used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpTy-RDHX1vR"
      },
      "outputs": [],
      "source": [
        "def benchmark_inference(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_lengths: List[int] = [64, 128, 256, 512],\n",
        "    gen_tokens: int = 32,\n",
        "    warmup_runs: int = 2,\n",
        "    benchmark_runs: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Benchmark prefill and decode times for various prompt lengths.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for prompt_len in prompt_lengths:\n",
        "        print(f\"\\nBenchmarking prompt_len={prompt_len}...\")\n",
        "\n",
        "        # Create prompt of specific length\n",
        "        base_prompt = \"Explain the following concept in detail: \"\n",
        "        padding = \"word \" * (prompt_len // 2)\n",
        "        prompt = base_prompt + padding\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=prompt_len)\n",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "        actual_prompt_len = inputs['input_ids'].shape[1]\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(warmup_runs):\n",
        "            with torch.no_grad():\n",
        "                _ = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=gen_tokens,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Benchmark\n",
        "        times = []\n",
        "        for _ in range(benchmark_runs):\n",
        "            torch.cuda.synchronize()\n",
        "            start = time.perf_counter()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=gen_tokens,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            end = time.perf_counter()\n",
        "            times.append(end - start)\n",
        "\n",
        "        avg_time = np.mean(times)\n",
        "        std_time = np.std(times)\n",
        "\n",
        "        # Calculate achieved performance\n",
        "        prefill_flops = calculate_prefill_flops(mistral, actual_prompt_len)['total']\n",
        "        decode_flops = calculate_decode_flops(mistral, actual_prompt_len + gen_tokens // 2)['total'] * gen_tokens\n",
        "        total_flops = prefill_flops + decode_flops\n",
        "\n",
        "        achieved_tflops = (total_flops / avg_time) / 1e12\n",
        "        tokens_per_sec = gen_tokens / avg_time\n",
        "\n",
        "        result = {\n",
        "            'prompt_len': actual_prompt_len,\n",
        "            'gen_tokens': gen_tokens,\n",
        "            'avg_time': avg_time,\n",
        "            'std_time': std_time,\n",
        "            'prefill_flops': prefill_flops,\n",
        "            'decode_flops': decode_flops,\n",
        "            'total_flops': total_flops,\n",
        "            'achieved_tflops': achieved_tflops,\n",
        "            'tokens_per_sec': tokens_per_sec\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"  Prompt: {actual_prompt_len} tokens, Gen: {gen_tokens} tokens\")\n",
        "        print(f\"  Time: {avg_time:.3f}s ¬± {std_time:.3f}s\")\n",
        "        print(f\"  Achieved: {achieved_tflops:.2f} TFLOPS\")\n",
        "        print(f\"  Throughput: {tokens_per_sec:.1f} tokens/sec\")\n",
        "\n",
        "        # Clear cache\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run benchmarks\n",
        "print(\"‚ïê\" * 60)\n",
        "print(\"Running Inference Benchmarks\")\n",
        "print(\"‚ïê\" * 60)\n",
        "\n",
        "benchmark_results = benchmark_inference(\n",
        "    model, tokenizer,\n",
        "    prompt_lengths=[64, 128, 256, 512],\n",
        "    gen_tokens=32,\n",
        "    warmup_runs=2,\n",
        "    benchmark_runs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlJlazzPX1vR"
      },
      "source": [
        "---\n",
        "## 7. Plot the Roofline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp1CPaaGX1vR"
      },
      "outputs": [],
      "source": [
        "def plot_roofline(t4_specs: T4Specs, benchmark_results: List[Dict], mistral_config: MistralConfig):\n",
        "    \"\"\"\n",
        "    Create a roofline plot with benchmark results.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Roofline parameters\n",
        "    peak_fp16 = t4_specs.peak_fp16_tflops\n",
        "    mem_bw = t4_specs.memory_bandwidth_gb_s\n",
        "    ridge_point = t4_specs.ridge_point_fp16\n",
        "\n",
        "    # Create roofline\n",
        "    oi_range = np.logspace(-1, 4, 500)\n",
        "\n",
        "    # Memory-bound region: performance = bandwidth * OI\n",
        "    memory_bound = mem_bw * oi_range / 1000  # Convert to TFLOPS\n",
        "\n",
        "    # Compute-bound region: performance = peak\n",
        "    compute_bound = np.full_like(oi_range, peak_fp16)\n",
        "\n",
        "    # Actual roofline is minimum of both\n",
        "    roofline = np.minimum(memory_bound, compute_bound)\n",
        "\n",
        "    # Plot roofline\n",
        "    ax.loglog(oi_range, roofline, 'b-', linewidth=3, label=f'T4 Roofline (FP16)')\n",
        "\n",
        "    # Add practical roofline (70% efficiency)\n",
        "    practical_roofline = roofline * t4_specs.practical_efficiency\n",
        "    ax.loglog(oi_range, practical_roofline, 'b--', linewidth=2, alpha=0.5,\n",
        "              label=f'Practical (~{int(t4_specs.practical_efficiency*100)}% efficiency)')\n",
        "\n",
        "    # Mark ridge point\n",
        "    ax.axvline(ridge_point, color='gray', linestyle=':', alpha=0.7)\n",
        "    ax.annotate(f'Ridge Point\\n({ridge_point:.0f} F/B)',\n",
        "                xy=(ridge_point, peak_fp16),\n",
        "                xytext=(ridge_point*2, peak_fp16*0.7),\n",
        "                fontsize=10, ha='left',\n",
        "                arrowprops=dict(arrowstyle='->', color='gray', alpha=0.7))\n",
        "\n",
        "    # Plot theoretical points for prefill\n",
        "    prefill_colors = plt.cm.Greens(np.linspace(0.4, 0.9, 5))\n",
        "    for i, seq_len in enumerate([128, 256, 512, 1024, 2048]):\n",
        "        flops, mem, oi = calculate_operational_intensity(mistral_config, seq_len, phase=\"prefill\")\n",
        "        # Theoretical max performance at this OI\n",
        "        theoretical_perf = min(peak_fp16, mem_bw * oi / 1000)\n",
        "        ax.scatter([oi], [theoretical_perf * 0.5], c=[prefill_colors[i]], s=100, marker='s',\n",
        "                   edgecolors='black', linewidths=1, zorder=5)\n",
        "        ax.annotate(f'Prefill {seq_len}', xy=(oi, theoretical_perf * 0.5),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "    # Plot theoretical points for decode\n",
        "    decode_colors = plt.cm.Oranges(np.linspace(0.4, 0.9, 5))\n",
        "    for i, kv_len in enumerate([128, 256, 512, 1024, 2048]):\n",
        "        flops, mem, oi = calculate_operational_intensity(mistral_config, kv_len, phase=\"decode\")\n",
        "        theoretical_perf = min(peak_fp16, mem_bw * oi / 1000)\n",
        "        ax.scatter([oi], [theoretical_perf * 0.4], c=[decode_colors[i]], s=100, marker='^',\n",
        "                   edgecolors='black', linewidths=1, zorder=5)\n",
        "        ax.annotate(f'Decode {kv_len}', xy=(oi, theoretical_perf * 0.4),\n",
        "                    xytext=(5, -10), textcoords='offset points', fontsize=8)\n",
        "\n",
        "    # Plot actual benchmark results\n",
        "    if benchmark_results:\n",
        "        for result in benchmark_results:\n",
        "            # Estimate OI from benchmark\n",
        "            mem_traffic = calculate_memory_traffic(\n",
        "                mistral_config, result['prompt_len'], phase=\"prefill\"\n",
        "            )['total']\n",
        "            oi = result['total_flops'] / mem_traffic\n",
        "\n",
        "            ax.scatter([oi], [result['achieved_tflops']], c='red', s=200, marker='*',\n",
        "                       edgecolors='darkred', linewidths=1.5, zorder=10,\n",
        "                       label=f'Measured (prompt={result[\"prompt_len\"]})')\n",
        "\n",
        "    # Formatting\n",
        "    ax.set_xlabel('Operational Intensity (FLOPs/Byte)', fontsize=12)\n",
        "    ax.set_ylabel('Performance (TFLOPS)', fontsize=12)\n",
        "    ax.set_title('Roofline Analysis: Mistral-7B on NVIDIA T4', fontsize=14, fontweight='bold')\n",
        "\n",
        "    ax.set_xlim(0.1, 10000)\n",
        "    ax.set_ylim(0.1, 100)\n",
        "\n",
        "    ax.grid(True, which='both', alpha=0.3)\n",
        "    ax.legend(loc='lower right', fontsize=9)\n",
        "\n",
        "    # Add annotations for regions\n",
        "    ax.text(0.5, 0.3, 'Memory\\nBound', fontsize=14, alpha=0.5, ha='center', transform=ax.transAxes)\n",
        "    ax.text(0.85, 0.7, 'Compute\\nBound', fontsize=14, alpha=0.5, ha='center', transform=ax.transAxes)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Create the roofline plot\n",
        "fig = plot_roofline(t4, benchmark_results, mistral)\n",
        "plt.savefig('roofline_mistral_t4.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Plot saved as 'roofline_mistral_t4.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sy13VbyX1vS"
      },
      "source": [
        "---\n",
        "## 8. Detailed Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kaeO9WoX1vS"
      },
      "outputs": [],
      "source": [
        "def plot_performance_breakdown(benchmark_results: List[Dict]):\n",
        "    \"\"\"\n",
        "    Visualize performance metrics from benchmarks.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    prompt_lens = [r['prompt_len'] for r in benchmark_results]\n",
        "\n",
        "    # 1. Throughput vs Prompt Length\n",
        "    ax1 = axes[0, 0]\n",
        "    throughputs = [r['tokens_per_sec'] for r in benchmark_results]\n",
        "    ax1.bar(range(len(prompt_lens)), throughputs, color='steelblue', edgecolor='black')\n",
        "    ax1.set_xticks(range(len(prompt_lens)))\n",
        "    ax1.set_xticklabels(prompt_lens)\n",
        "    ax1.set_xlabel('Prompt Length (tokens)')\n",
        "    ax1.set_ylabel('Throughput (tokens/sec)')\n",
        "    ax1.set_title('Generation Throughput vs Prompt Length')\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # 2. Achieved TFLOPS\n",
        "    ax2 = axes[0, 1]\n",
        "    achieved = [r['achieved_tflops'] for r in benchmark_results]\n",
        "    ax2.bar(range(len(prompt_lens)), achieved, color='coral', edgecolor='black')\n",
        "    ax2.axhline(t4.peak_fp16_tflops, color='red', linestyle='--', label=f'T4 Peak FP16 ({t4.peak_fp16_tflops} TFLOPS)')\n",
        "    ax2.axhline(t4.peak_fp16_tflops * 0.7, color='orange', linestyle=':', label='70% efficiency')\n",
        "    ax2.set_xticks(range(len(prompt_lens)))\n",
        "    ax2.set_xticklabels(prompt_lens)\n",
        "    ax2.set_xlabel('Prompt Length (tokens)')\n",
        "    ax2.set_ylabel('Achieved Performance (TFLOPS)')\n",
        "    ax2.set_title('Achieved vs Peak Performance')\n",
        "    ax2.legend()\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # 3. Time Breakdown\n",
        "    ax3 = axes[1, 0]\n",
        "    times = [r['avg_time'] for r in benchmark_results]\n",
        "    stds = [r['std_time'] for r in benchmark_results]\n",
        "    ax3.bar(range(len(prompt_lens)), times, yerr=stds, color='mediumseagreen',\n",
        "            edgecolor='black', capsize=5)\n",
        "    ax3.set_xticks(range(len(prompt_lens)))\n",
        "    ax3.set_xticklabels(prompt_lens)\n",
        "    ax3.set_xlabel('Prompt Length (tokens)')\n",
        "    ax3.set_ylabel('Inference Time (seconds)')\n",
        "    ax3.set_title('Total Inference Time')\n",
        "    ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # 4. FLOP Breakdown\n",
        "    ax4 = axes[1, 1]\n",
        "    prefill_flops = [r['prefill_flops']/1e12 for r in benchmark_results]\n",
        "    decode_flops = [r['decode_flops']/1e12 for r in benchmark_results]\n",
        "\n",
        "    x = np.arange(len(prompt_lens))\n",
        "    width = 0.35\n",
        "\n",
        "    ax4.bar(x - width/2, prefill_flops, width, label='Prefill', color='royalblue', edgecolor='black')\n",
        "    ax4.bar(x + width/2, decode_flops, width, label='Decode', color='darkorange', edgecolor='black')\n",
        "    ax4.set_xticks(x)\n",
        "    ax4.set_xticklabels(prompt_lens)\n",
        "    ax4.set_xlabel('Prompt Length (tokens)')\n",
        "    ax4.set_ylabel('FLOPs (TFLOPS)')\n",
        "    ax4.set_title('FLOP Distribution: Prefill vs Decode')\n",
        "    ax4.legend()\n",
        "    ax4.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "if benchmark_results:\n",
        "    fig = plot_performance_breakdown(benchmark_results)\n",
        "    plt.savefig('performance_breakdown.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hZKA5pLX1vS"
      },
      "source": [
        "---\n",
        "## 9. Analysis Summary and Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjeWsgUPX1vS"
      },
      "outputs": [],
      "source": [
        "def generate_analysis_report(t4_specs: T4Specs, benchmark_results: List[Dict], mistral_config: MistralConfig):\n",
        "    \"\"\"\n",
        "    Generate a summary report with optimization recommendations.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"‚ïê\" * 70)\n",
        "    print(\"ROOFLINE ANALYSIS REPORT: Mistral-7B on NVIDIA T4\")\n",
        "    print(\"‚ïê\" * 70)\n",
        "\n",
        "    # Key findings\n",
        "    print(\"\\nüìä KEY FINDINGS\")\n",
        "    print(\"‚îÄ\" * 70)\n",
        "\n",
        "    # Calculate average efficiency\n",
        "    if benchmark_results:\n",
        "        avg_achieved = np.mean([r['achieved_tflops'] for r in benchmark_results])\n",
        "        efficiency = avg_achieved / t4_specs.peak_fp16_tflops * 100\n",
        "        avg_throughput = np.mean([r['tokens_per_sec'] for r in benchmark_results])\n",
        "\n",
        "        print(f\"  Average achieved performance: {avg_achieved:.2f} TFLOPS\")\n",
        "        print(f\"  Hardware utilization: {efficiency:.1f}% of peak FP16\")\n",
        "        print(f\"  Average throughput: {avg_throughput:.1f} tokens/sec\")\n",
        "\n",
        "    # Bottleneck analysis\n",
        "    print(\"\\nüîç BOTTLENECK ANALYSIS\")\n",
        "    print(\"‚îÄ\" * 70)\n",
        "\n",
        "    # Check decode OI\n",
        "    _, _, decode_oi = calculate_operational_intensity(mistral_config, 512, phase=\"decode\")\n",
        "    _, _, prefill_oi = calculate_operational_intensity(mistral_config, 512, phase=\"prefill\")\n",
        "\n",
        "    print(f\"  Prefill OI (512 tokens): {prefill_oi:.1f} FLOPs/Byte\")\n",
        "    print(f\"  Decode OI (512 KV cache): {decode_oi:.1f} FLOPs/Byte\")\n",
        "    print(f\"  T4 Ridge Point (FP16): {t4_specs.ridge_point_fp16:.1f} FLOPs/Byte\")\n",
        "\n",
        "    if decode_oi < t4_specs.ridge_point_fp16:\n",
        "        print(\"\\n  ‚ö†Ô∏è  DECODE PHASE IS MEMORY-BOUND\")\n",
        "        print(\"      The decode phase (token generation) is limited by memory bandwidth.\")\n",
        "        print(\"      This is typical for autoregressive LLM inference.\")\n",
        "\n",
        "    if prefill_oi > t4_specs.ridge_point_fp16:\n",
        "        print(\"\\n  ‚úÖ PREFILL PHASE IS COMPUTE-BOUND (for longer prompts)\")\n",
        "        print(\"      The prefill phase can better utilize Tensor Cores.\")\n",
        "\n",
        "    # Optimization recommendations\n",
        "    print(\"\\nüí° OPTIMIZATION RECOMMENDATIONS\")\n",
        "    print(\"‚îÄ\" * 70)\n",
        "\n",
        "    recommendations = [\n",
        "        (\"Increase Batch Size\",\n",
        "         \"Batching multiple requests improves arithmetic intensity and GPU utilization.\",\n",
        "         \"Memory-bound decode becomes more efficient with larger batches.\"),\n",
        "\n",
        "        (\"Use Quantization\",\n",
        "         \"INT8/INT4 quantization reduces memory traffic and increases effective bandwidth.\",\n",
        "         \"T4 has 130 INT8 TOPS vs 65 FP16 TFLOPS - 2x theoretical improvement.\"),\n",
        "\n",
        "        (\"Enable Flash Attention\",\n",
        "         \"Reduces memory traffic for attention computation.\",\n",
        "         \"Fuses operations to minimize HBM access.\"),\n",
        "\n",
        "        (\"Use Continuous Batching\",\n",
        "         \"Dynamically batch requests to maximize GPU utilization.\",\n",
        "         \"Frameworks: vLLM, TensorRT-LLM, Text Generation Inference.\"),\n",
        "\n",
        "        (\"KV Cache Optimization\",\n",
        "         \"PagedAttention reduces memory fragmentation.\",\n",
        "         \"Allows serving more concurrent requests.\"),\n",
        "\n",
        "        (\"Speculative Decoding\",\n",
        "         \"Use a smaller draft model to predict multiple tokens.\",\n",
        "         \"Can improve throughput by 2-3x for memory-bound scenarios.\")\n",
        "    ]\n",
        "\n",
        "    for i, (title, desc, detail) in enumerate(recommendations, 1):\n",
        "        print(f\"\\n  {i}. {title}\")\n",
        "        print(f\"     {desc}\")\n",
        "        print(f\"     ‚Üí {detail}\")\n",
        "\n",
        "    # Hardware comparison\n",
        "    print(\"\\nüìà HARDWARE SCALING NOTES\")\n",
        "    print(\"‚îÄ\" * 70)\n",
        "    print(\"  For memory-bound workloads like LLM decode:\")\n",
        "    print(\"  ‚Ä¢ A100 (1.5 TB/s BW) would be ~4.7x faster than T4 (320 GB/s)\")\n",
        "    print(\"  ‚Ä¢ H100 (3.35 TB/s BW) would be ~10x faster than T4\")\n",
        "    print(\"  ‚Ä¢ For compute-bound prefill, gains scale with TFLOPS instead\")\n",
        "\n",
        "    print(\"\\n\" + \"‚ïê\" * 70)\n",
        "    print(\"END OF REPORT\")\n",
        "    print(\"‚ïê\" * 70)\n",
        "\n",
        "generate_analysis_report(t4, benchmark_results, mistral)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN-Vfv2dX1vS"
      },
      "source": [
        "---\n",
        "## 10. Interactive Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIRUULAWX1vS"
      },
      "outputs": [],
      "source": [
        "# Interactive: Test different batch sizes (if memory allows)\n",
        "\n",
        "def explore_batch_scaling(model, tokenizer, batch_sizes=[1, 2, 4], prompt_len=128, gen_tokens=16):\n",
        "    \"\"\"\n",
        "    Explore how batch size affects throughput.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"‚ïê\" * 60)\n",
        "    print(\"BATCH SIZE SCALING ANALYSIS\")\n",
        "    print(\"‚ïê\" * 60)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        try:\n",
        "            # Create batched input\n",
        "            prompt = \"Explain machine learning in simple terms. \" * (prompt_len // 10)\n",
        "            inputs = tokenizer([prompt] * batch_size, return_tensors=\"pt\",\n",
        "                              padding=True, truncation=True, max_length=prompt_len)\n",
        "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "            # Warmup\n",
        "            with torch.no_grad():\n",
        "                _ = model.generate(**inputs, max_new_tokens=gen_tokens, do_sample=False,\n",
        "                                   pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "            # Benchmark\n",
        "            torch.cuda.synchronize()\n",
        "            start = time.perf_counter()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _ = model.generate(**inputs, max_new_tokens=gen_tokens, do_sample=False,\n",
        "                                   pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "            elapsed = time.perf_counter() - start\n",
        "\n",
        "            total_tokens = batch_size * gen_tokens\n",
        "            throughput = total_tokens / elapsed\n",
        "\n",
        "            results.append({\n",
        "                'batch_size': batch_size,\n",
        "                'time': elapsed,\n",
        "                'throughput': throughput\n",
        "            })\n",
        "\n",
        "            print(f\"  Batch {batch_size}: {throughput:.1f} tokens/sec ({elapsed:.3f}s for {total_tokens} tokens)\")\n",
        "\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(f\"  Batch {batch_size}: ‚ùå OOM - not enough GPU memory\")\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    if len(results) > 1:\n",
        "        scaling = results[-1]['throughput'] / results[0]['throughput']\n",
        "        print(f\"\\n  Throughput scaling (batch {results[-1]['batch_size']} vs {results[0]['batch_size']}): {scaling:.2f}x\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Uncomment to run batch scaling analysis\n",
        "# batch_results = explore_batch_scaling(model, tokenizer, batch_sizes=[1, 2], prompt_len=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq08TE4jX1vS"
      },
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **T4 Hardware Limits**: 65 TFLOPS (FP16), 320 GB/s memory bandwidth\n",
        "2. **Roofline Analysis**: Visualized compute vs memory boundedness\n",
        "3. **Key Insight**: LLM decode is **memory-bound** on T4 (and most GPUs)\n",
        "4. **Optimizations**: Batching, quantization, and specialized inference engines help\n",
        "\n",
        "For production deployment, consider:\n",
        "- **vLLM** or **TensorRT-LLM** for optimized inference\n",
        "- **Quantization** (AWQ, GPTQ) for better memory efficiency\n",
        "- **Larger GPUs** (A100, H100) if memory bandwidth is the bottleneck"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "54a0434cba2648babf5eaf171eb10a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4357ca5b0211472c8946df0b950710ef",
              "IPY_MODEL_3d148d4880ac4578a9ab6b3f9a2eda8e",
              "IPY_MODEL_97873d730ca84a9c9fe1e92579c18d96"
            ],
            "layout": "IPY_MODEL_de2190c7d2504b5d84ef276aca25eed7"
          }
        },
        "4357ca5b0211472c8946df0b950710ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8f377a4f091472398f9cea69a05ebda",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ce47db5bf14d42de8b6f806ee1071710",
            "value": "Loading‚Äáweights:‚Äá‚Äá91%"
          }
        },
        "3d148d4880ac4578a9ab6b3f9a2eda8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78e2c9be96ac42799e4a2ca3f8576b51",
            "max": 291,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44008ada972d4e5a89e81110bc27fa19",
            "value": 266
          }
        },
        "97873d730ca84a9c9fe1e92579c18d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c683b57f6664ba4842aa26c8f08a485",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7193531b1ef34df8afd64d27b67e0dd4",
            "value": "‚Äá266/291‚Äá[00:54&lt;00:04,‚Äá‚Äá5.59it/s,‚ÄáMaterializing‚Äáparam=model.layers.29.mlp.gate_proj.weight]"
          }
        },
        "de2190c7d2504b5d84ef276aca25eed7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8f377a4f091472398f9cea69a05ebda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce47db5bf14d42de8b6f806ee1071710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78e2c9be96ac42799e4a2ca3f8576b51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44008ada972d4e5a89e81110bc27fa19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c683b57f6664ba4842aa26c8f08a485": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7193531b1ef34df8afd64d27b67e0dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}